import os
import streamlit as st
import pdfplumber
import json
import openai  # Import the OpenAI library

from langchain import OpenAI, VectorDBQA
from langchain.document_loaders import DirectoryLoader
from langchain.embeddings.openai import OpenAIEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import Chroma

def cache_response(question, response):
    with open("cache.json", "r") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            data = {}
    
    data[question] = response
    
    with open("cache.json", "w") as f:
        json.dump(data, f)

def get_cached_response(question):
    with open("cache.json", "r") as f:
        try:
            data = json.load(f)
        except json.JSONDecodeError:
            data = {}
        
        if question in data:
            return data[question]
    return None

def clear_cache():
    with open("cache.json", "w") as f:
        data = {}
        json.dump(data, f)

def load_data_from_local_directory(directory_path, user_query):
    data = []
    for filename in os.listdir(directory_path):
        if filename.lower().endswith(".pdf"):
            first_word = filename.lower().split()[0]
            if first_word in user_query.lower():
                with pdfplumber.open(os.path.join(directory_path, filename)) as pdf:
                    for page in pdf.pages:
                        table = page.extract_table()
                        if table:
                            header = table[0]
                            for row in table[1:]:
                                data.append({header[i]: row[i] for i in range(len(header))})
    return data

def generate_table_from_data(data):
    if not data:
        return None

    # Manually construct the table HTML
    table_html = "<table class='no-index'><thead><tr>"
    header = list(data[0].keys())
    for col in header:
        table_html += f"<th>{col}</th>"
    table_html += "</tr></thead><tbody>"

    for row in data:
        table_html += "<tr>"
        for col in header:
            table_html += f"<td>{row[col]}</td>"
        table_html += "</tr>"
    
    table_html += "</tbody></table>"

    return table_html

# Function to get the response directly from GPT-3.5 (the advanced AI model)
def get_advanced_ai_response(question):
    # Set up your OpenAI API key
    openai.api_key = "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"

    # Call the OpenAI API to get the response from GPT-3.5
    response = openai.Completion.create(
        engine="text-davinci-002",  # GPT-3.5 engine
        prompt=question,
        max_tokens=100  # Adjust the number of tokens as needed
    )

    return response.choices[0].text.strip()

def main():
    os.environ["OPENAI_API_KEY"] = "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"  # Set the OpenAI API key here
    data_directory = "C:/Users/Ankur Sharma/Desktop/datachatbot"

    txt_loader = DirectoryLoader(data_directory, glob='**/*.txt')
    pdf_loader = DirectoryLoader(data_directory, glob='**/*.pdf')

    try:
        txt_docs = txt_loader.load()
        pdf_docs = pdf_loader.load()
        docs = txt_docs + pdf_docs
    except FileNotFoundError:
        st.error("The provided directory path or file pattern is not valid.")
        return

    char_text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    doc_texts = char_text_splitter.split_documents(docs)

    openAI_embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])
    vStore = Chroma.from_documents(doc_texts, openAI_embeddings)
    model = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type="stuff", vectorstore=vStore)

    st.title("Welcome to AI Training Chatbot")
    st.sidebar.title("Actions")

    if st.sidebar.button("Clear Cache"):
        clear_cache()

    st.sidebar.markdown('<p style="font-size: 12px; font-weight: bold;">Clear Cache to get updated data.</p>', unsafe_allow_html=True)

    # Initialize session state to store the state of elements
    if "selected_prompt" not in st.session_state:
        st.session_state.selected_prompt = ""

    if "refer_advanced_ai" not in st.session_state:
        st.session_state.refer_advanced_ai = "No"

    # Get the selected prompt from the smart prompt buttons
    if st.button("Elden training plan in Table format"):
        st.session_state.selected_prompt = "Elden training plan in Table format"

    if st.button("Testing training plan in table format"):
        st.session_state.selected_prompt = "Testing training plan in table format"

    # Text input for user to provide a custom question
    custom_question = st.text_area("Please enter your question:", value=st.session_state.selected_prompt)

    # Use the selected prompt if available, otherwise use the custom question
    question = custom_question.strip()

    # Add Yes/No radio buttons for "Refer our advanced AI"
    refer_advanced_ai = st.radio("Want to Use AI model?", options=["Yes", "No"], index=1)  # Set default value to "No" (index=1)
    st.session_state.refer_advanced_ai = refer_advanced_ai

    # Show the checkbox only if "Yes" is selected
    if refer_advanced_ai == "Yes":
        checkbox_selected = st.checkbox("Please check the checkbox")

    if st.button("Get Answer"):
        question = question.strip()
        if not question:
            st.warning("Please enter a question.")
            return

        if refer_advanced_ai == "Yes":  # Check the value of the radio button
            response = get_advanced_ai_response(question)
            st.write(response)
        elif "table" in question.lower():
            cached_table_response = get_cached_response(question)
            if cached_table_response is not None:
                st.markdown("## Cached Response (Table)")
                st.markdown(generate_table_from_data(cached_table_response), unsafe_allow_html=True)
            else:
                data = load_data_from_local_directory(data_directory, question)
                table_response = generate_table_from_data(data)
                if table_response is not None:
                    st.markdown(table_response, unsafe_allow_html=True)
                    cache_response(question, data)
                else:
                    st.warning("No data available.")
        else:
            try:
                cached_response = get_cached_response(question)
                if cached_response is not None:
                    st.markdown("## Cached Response")
                    st.write(cached_response)
                else:
                    answer = model.run(question)
                    st.write(answer)
                    cache_response(question, answer)
            except Exception as e:
                st.error(f"An error occurred while running the model: {e}")
                return

if __name__ == "__main__":
    main()
